{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import seed\n",
    "from copy import copy\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from os import path\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import pathlib\n",
    "\n",
    "import datetime\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from recommender_module.price_crawler import build_ta_features, get_period_and_frequency\n",
    "from recommender_module.rnn_utils import get_model, pd_to_tensor\n",
    "from recommender_module.rnn_trend import to_csv_lines"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enable CUDA if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda:0'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare Global variables and config generation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = ['1D-close', '7D-close', '30D-close']\n",
    "\n",
    "# Number of trials\n",
    "TRIAL = 10\n",
    "\n",
    "# TA features to be used\n",
    "TA_FEATURES = ['ROC', 'MOM', 'EMA', 'SMA', 'VAR', 'MACD', 'ADX', 'RSI']\n",
    "\n",
    "SIZE_RANGE = [32, 64, 96]                     # Size of hidden layers within the model\n",
    "DELTA_RANGE = ['24H']      # Possible duration of each period\n",
    "MODEL_RANGE = ['LSTM', 'GRU', 'RNN']          # Possible Models\n",
    "# MODEL_RANGE = ['LSTM']          # Possible Models\n",
    "NORMALIZE = ['Normal', 'MinMax']              # Possible data normalization method\n",
    "LR_RANGE = [0.001]                      # Possible learning rates\n",
    "# ROLL_RANGE = [[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 20, 25, 30]]\n",
    "ROLL_RANGE = [list(range(1,30)), list(range(1,60, 2))]\n",
    "NUMBER_BLOCKS = [3, 5]\n",
    "\n",
    "def generate_config(device='cpu'):\n",
    "    seed()\n",
    "\n",
    "    data_iter = len(list(itertools.product(MODEL_RANGE, SIZE_RANGE, LR_RANGE, NUMBER_BLOCKS)))\n",
    "    all_configs = itertools.product(NORMALIZE, DELTA_RANGE, ROLL_RANGE, MODEL_RANGE, SIZE_RANGE, LR_RANGE, NUMBER_BLOCKS)\n",
    "    configs = [{\n",
    "        'roll': c[2],\n",
    "        'delta': c[1],\n",
    "        'model': c[3],\n",
    "        'norm': c[0],\n",
    "        'hidden': c[4],\n",
    "        'lr': c[5],\n",
    "        'n_blocks': c[6],\n",
    "        'targets': 1,\n",
    "        'device': device,\n",
    "        } for c in all_configs]\n",
    "\n",
    "    target = [copy(TARGET) for _ in configs]\n",
    "\n",
    "    features = [['open', 'close', 'high', 'low', 'volume'] + [s.lower() for s in TA_FEATURES] for _ in configs]\n",
    "    for k, feature in enumerate(features):\n",
    "        feature += [j + '-r' + str(i) for j in feature for i in configs[k]['roll']]\n",
    "\n",
    "    return configs, target, features, data_iter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions modified for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_features(address, freq='1D', ta_list=None, ys=['1D-close'], roll=[1]):\n",
    "\n",
    "    '''\n",
    "    To build features for the target address.\n",
    "    Input(address <str>) -> DataFrame\n",
    "    Read the craweled Dataframe of the specified address and add feature columns to it\n",
    "    '''\n",
    "\n",
    "    file_dir = \"price_data/\" + address + '.pkl'\n",
    "    with open(file_dir, 'rb') as f:\n",
    "        df, update_time = pickle.load(f)\n",
    "\n",
    "    x_df = build_ta_features(df, freq=freq, ta_list=ta_list)\n",
    "\n",
    "    # Adding rolling features\n",
    "    columns = copy(x_df.columns)\n",
    "    for i in roll:\n",
    "        for c in columns:\n",
    "            x_df[c + '-r' + str(i)] = x_df[c].shift(i)\n",
    "\n",
    "    if ys is None:\n",
    "        y_df = None\n",
    "    else:\n",
    "        # Adding labels\n",
    "        y_df = x_df.copy() # Just to store x_df's index into y_df\n",
    "        for target in ys:\n",
    "            interval, feature = target.split('-')\n",
    "            p, f = get_period_and_frequency(interval)\n",
    "            y_df[target] = (x_df[feature].shift(periods=-p, freq=f) - x_df[feature]) > 0\n",
    "        y_df = y_df.drop(columns=x_df.columns) # Features in x_df are dropped\n",
    "\n",
    "        # Dropping all the np.inf and np.nan values in dataframes\n",
    "        x_df = x_df.replace(np.inf, np.nan).dropna(how='all', axis=1).dropna(how='any', axis=0)\n",
    "        y_df = y_df.replace(np.inf, np.nan).dropna(how='all', axis=1).dropna(how='any', axis=0)\n",
    "\n",
    "        # Truncating series to make sure their indices are the same\n",
    "        if len(x_df) > len(y_df):\n",
    "            x_df = x_df.loc[y_df.index]\n",
    "        else:\n",
    "            y_df = y_df.loc[x_df.index]\n",
    "\n",
    "        # Converting it to pd.Series if there is only one prediction target column\n",
    "        if len(y_df.columns) == 1:\n",
    "            y_df = y_df.iloc[:, 0] # Convert it to pd.Series\n",
    "\n",
    "    return x_df, y_df, x_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(address, target, cfg):\n",
    "    X, y, columns = build_features(address, freq=cfg['delta'], ta_list=TA_FEATURES, ys=target, roll=cfg['roll'])\n",
    "    \n",
    "    inp = len(X.columns)\n",
    "\n",
    "    if cfg['norm'] == 'MinMax':\n",
    "        X = (X - X.min()) / (X.max() - X.min())\n",
    "    if cfg['norm'] == 'Normal':\n",
    "        X = (X - X.mean()) / X.std()\n",
    "\n",
    "    X, y = pd_to_tensor(X, y, device)\n",
    "    y = y.unsqueeze(-1)\n",
    "    train_x, val_x, train_y, val_y = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    train_x = X\n",
    "    train_y = y\n",
    "\n",
    "    return train_x, train_y, val_x, val_y, columns, inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data, cfg, target, device='cpu', finetune_model=None):\n",
    "\n",
    "    '''\n",
    "    Input(address <str>, configuration <dict>, features <list>, target <dict>, log <file>) -> list\n",
    "    This function gives a list of trained model for each time horizon using the configuration above\n",
    "    '''\n",
    "    train_x, train_y, val_x, val_y, _, _ = data\n",
    "    original_x = train_x.detach().clone()\n",
    "\n",
    "    if finetune_model != None:\n",
    "        rnn = finetune_model\n",
    "    else:\n",
    "        rnn = get_model(cfg)\n",
    "\n",
    "    optim = torch.optim.Adam(rnn.parameters(), lr=cfg['lr'])\n",
    "    criterion = nn.BCELoss()\n",
    "    i = 1\n",
    "    overfit = 0\n",
    "    previous_loss = 999\n",
    "    in_long_run = False\n",
    "    previous_train_loss = 999\n",
    "    previous_acc = 0\n",
    "    while True:\n",
    "        try:\n",
    "            optim.zero_grad()\n",
    "            train_pred = rnn(train_x)\n",
    "            loss = criterion(train_pred, train_y)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            val_pred = rnn(val_x)\n",
    "            val_acc = ((val_pred > 0.5) == val_y).float()\n",
    "            val_acc = torch.mean(val_acc, dim=0).tolist()[0][0]\n",
    "\n",
    "            if val_acc < previous_acc:\n",
    "                overfit += 1\n",
    "            else:\n",
    "                overfit = 0\n",
    "            if overfit > 3:\n",
    "                break\n",
    "\n",
    "            previous_acc = val_acc\n",
    "        except RuntimeError:\n",
    "            return None, None, None, None\n",
    "\n",
    "        i += 1\n",
    "        if i > 4000:\n",
    "            break\n",
    "        if i % 1000 == 0:\n",
    "            val_acc = ((val_pred > 0.5) == val_y).float()\n",
    "            val_acc = torch.mean(val_acc, dim=0).tolist()[0][0]\n",
    "            print('long run', i // 1000, 'best val:', val_acc)\n",
    "            in_long_run = True\n",
    "\n",
    "    train_pred = rnn(original_x)\n",
    "    train_acc = ((train_pred > 0.5) == train_y).float()\n",
    "    train_acc = torch.mean(train_acc, dim=0).tolist()[0]\n",
    "    val_pred = rnn(val_x)\n",
    "    val_acc = ((val_pred > 0.5) == val_y).float()\n",
    "    val_acc = torch.mean(val_acc, dim=0).tolist()[0]\n",
    "    lines = to_csv_lines(cfg, target, train_acc, val_acc)\n",
    "    return rnn, lines, val_acc, in_long_run"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List all available data and already existing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_models = [f.stem.split('.')[0] for f in pathlib.Path('models/').iterdir()]\n",
    "\n",
    "data_files = pathlib.Path('price_data/').iterdir()\n",
    "new_data_files = [(f, datetime.date.fromtimestamp(f.stat().st_mtime)) for f in data_files]\n",
    "new_data_files = [f[0] for f in new_data_files if f[1] >= datetime.date.today() - datetime.timedelta(days=7)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train new models and fine tune existing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in new_data_files:    \n",
    "    address = f.stem\n",
    "        \n",
    "    file_name = f.stem + '_rnn.csv'\n",
    "    if path.exists('tuning_logs/' + file_name):\n",
    "        log = open('tuning_logs/' + file_name, 'a')\n",
    "        # continue\n",
    "    else:\n",
    "        log = open('tuning_logs/' + file_name, 'w')\n",
    "        log.write('TARGET, MODEL, DELTA, NORM, HIDDEN, LR, TRAIN, VAL\\n')\n",
    "\n",
    "    csv_lines = []\n",
    "    cfgs, targets, featureses, data_iter = generate_config(device)\n",
    "    for t in TARGET:\n",
    "        if address + '-' + t in existing_models:\n",
    "            # Fine tune model\n",
    "            print(\"Fine tuning\", address + '-' + t)\n",
    "            model, cfg, columns =  pickle.load(open(f\"models/{address}-{t}.pkl\", \"rb\"))\n",
    "            model = model.to(device)\n",
    "            data = load_data(address, [t], cfg)\n",
    "            model, lines, val_acc, long_run = train(data, cfg, [t], device, finetune_model=model)\n",
    "            pickle.dump([model.to('cpu'), cfg, columns], open(f\"models/{address}-{t}.pkl\", \"wb\")) \n",
    "            continue \n",
    "\n",
    "        best_model = None\n",
    "        best_config = None\n",
    "        best_val = 0\n",
    "        best_columns = [None]\n",
    "\n",
    "        train_iteration = list(zip(cfgs, featureses))\n",
    "        deltas = {\n",
    "            '1D-close': ['6H', '12H', '24H'],\n",
    "            '7D-close': ['12H', '24H', '1D'],\n",
    "            '30D-close': ['24H', '2D', '3D'],\n",
    "        }\n",
    "        for d in deltas[t]:\n",
    "            for i in (pbar := tqdm(range(len(train_iteration)), desc=f\"Training ({address +'-'+ t}) [{d}] - Best: {best_val:.3f} - # Col: {len(best_columns)}\")):\n",
    "                cfg, features = train_iteration[i]\n",
    "                cfg['delta'] = d\n",
    "\n",
    "                if i % data_iter == 0:\n",
    "                    try:\n",
    "                        data = load_data(address, [t], cfg)\n",
    "                    except ValueError as e:\n",
    "                        print(e)\n",
    "                        break\n",
    "                \n",
    "                cfg['input'] = data[-1]\n",
    "                columns = data[-2]\n",
    "                model, lines, val_acc, long_run = train(data, cfg, [t], device)\n",
    "                if model == None:\n",
    "                    continue\n",
    "                val_acc = np.mean(val_acc)\n",
    "\n",
    "                log.writelines(lines)\n",
    "                if (val_acc > best_val):\n",
    "                    best_val = val_acc\n",
    "                    best_model = model\n",
    "                    best_config = cfg\n",
    "\n",
    "                    best_columns = columns\n",
    "                    pbar.set_description(f\"Training ({address +'-'+ t}) [{d}] - Best: {best_val:.3f} - # Col: {len(best_columns)} - Roll: {cfg['roll']}\")\n",
    "                    if val_acc >= 0.999:\n",
    "                        break\n",
    "\n",
    "        if best_model != None:\n",
    "            pickle.dump([best_model.to('cpu'), best_config, best_columns], open(f\"models/{address}-{t}.pkl\", \"wb\"))                  \n",
    "\n",
    "    log.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
